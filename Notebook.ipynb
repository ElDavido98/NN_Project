{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElDavido98/Neural-Networks/blob/main/Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Final Project**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Contents**\n",
        "*   Preliminar Part\n",
        "*   Data Processing\n",
        "*   Utils\n",
        "*   Neural Networks\n",
        "*   Metrics\n",
        "*   Forecasting\n",
        "*   Pre\n",
        "*   Training\n",
        "*   Evaluation\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **N.B.**\n",
        "Using the basic version of Google Colab, it is not possible to run this notebook because the RAM memory is not sufficient. This problem persists even when decreasing the complexity of the models and decreasing the amount of data used for training."
      ],
      "metadata": {
        "id": "vuzhKuyn9qPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminar Part"
      ],
      "metadata": {
        "id": "wCql56yf5yVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.2.0\n",
        "!pip install numpy==1.26.4\n",
        "!pip install timm==0.9.16\n",
        "!pip install netCDF4==1.6.5\n",
        "!pip install scikit-learn==1.4.2\n",
        "!pip install matplotlib==3.8.4"
      ],
      "metadata": {
        "id": "opkYs-nKtPMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch\n",
        "import statistics\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import netCDF4\n",
        "import sklearn.preprocessing\n",
        "from timm.layers import PatchEmbed, DropPath"
      ],
      "metadata": {
        "id": "tmiXymPlfxmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "zQiXc59AnVIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "ERURcL_mUU9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils\n",
        "This section contains functions used by various classes."
      ],
      "metadata": {
        "id": "uFSTJu9z9vsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Neural_Networks/climate-learn\"\n",
        "\n",
        "single_folder = [\"toa_incident_solar_radiation_5.625deg\", \"2m_temperature_5.625deg\", \"10m_u_component_of_wind_5.625deg\",\n",
        "                 \"10m_v_component_of_wind_5.625deg\"]\n",
        "atmospheric_folder = [\"geopotential_5.625deg\", \"u_component_of_wind_5.625deg\", \"v_component_of_wind_5.625deg\",\n",
        "                      \"temperature_5.625deg\", \"specific_humidity_5.625deg\",\n",
        "                      \"relative_humidity_5.625deg\"]\n",
        "\n",
        "static_variable = \"constants_5.625deg\"\n",
        "single_variable = [\"toa_incident_solar_radiation_\", \"2m_temperature_\", \"10m_u_component_of_wind_\",\n",
        "                   \"10m_v_component_of_wind_\"]\n",
        "atmospheric_variable = [\"geopotential_\", \"u_component_of_wind_\", \"v_component_of_wind_\", \"temperature_\",\n",
        "                        \"specific_humidity_\", \"relative_humidity_\"]\n",
        "\n",
        "resolution = \"_5.625deg\"\n",
        "\n",
        "abbr = [\"lsm\", \"orography\", \"lat2d\", \"tisr\", \"t2m\", \"u10\", \"v10\", \"z\", \"u\", \"v\", \"t\", \"q\", \"r\"]\n",
        "\n",
        "levels = [50, 250, 500, 600, 700, 850, 925]\n",
        "lev_indexes = [0, 4, 7, 8, 9, 10, 11]\n",
        "\n",
        "low_bound_year_train, max_bound_year_train = 1980, 1981  # Original values from paper: 1979, 2016\n",
        "low_bound_year_val_test, max_bound_year_val_test = 1986, 1987   # First part for validation, second part for test\n",
        "\n",
        "low_year_train, max_year_train = 0, (max_bound_year_train - low_bound_year_train - 1)\n",
        "low_hour_train, max_hour_train = 0, 8759\n",
        "low_year_val, max_year_val = 0, (max_bound_year_val_test - low_bound_year_val_test - 1)\n",
        "low_hour_val, max_hour_val = 0, 4379\n",
        "low_year_test, max_year_test = 0, (max_bound_year_val_test - low_bound_year_val_test - 1)\n",
        "low_hour_test, max_hour_test = 0, 4379\n",
        "\n",
        "latitude_coordinates = [-87.1875, -81.5625, -75.9375, -70.3125, -64.6875, -59.0625, -53.4375, -47.8125, -42.1875,\n",
        "                        -36.5625, -30.9375, -25.3125, -19.6875, -14.0625, -8.4375, -2.8125, 2.8125, 8.4375, 14.0625,\n",
        "                        19.6875, 25.3125, 30.9375, 36.5625, 42.1875, 47.8125, 53.4375, 59.0625, 64.6875, 70.3125,\n",
        "                        75.9375, 81.5625, 87.1875]\n",
        "\n",
        "\n",
        "def return_to_image(x, patch_size, out_channels, img_size):\n",
        "    p = patch_size\n",
        "    c = out_channels\n",
        "    h = img_size[0] // p\n",
        "    w = img_size[1] // p\n",
        "    assert h * w == x.shape[1]\n",
        "    x = torch.reshape(x, shape=(x.shape[0], h, w, p, p, c)).to(device)\n",
        "    x = torch.einsum(\"nhwpqc->nchpwq\", x).to(device)\n",
        "    imgs = torch.reshape(x, shape=(x.shape[0], c, (h * p), (w * p))).to(device)\n",
        "    return imgs\n",
        "\n",
        "\n",
        "def make_layer(block, in_channels, out_channels, num_blocks, change=0):\n",
        "    layers = []\n",
        "    for i in range(num_blocks):\n",
        "        layers.append(block(in_channels, out_channels))\n",
        "        if change:\n",
        "            in_channels = out_channels\n",
        "    return nn.Sequential(*layers).to(device)\n",
        "\n",
        "\n",
        "def plot(name, linreg_baseline_rmse, linreg_baseline_acc, resnet_rmse, resnet_acc, unet_rmse, unet_acc, vit_rmse, vit_acc):\n",
        "    lead_time = [6, 24, 72, 120, 240]\n",
        "    # Create the figure and subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    # Plot RMSE on the first subplot\n",
        "    ax1.plot(lead_time, linreg_baseline_rmse, 'o-', label='Linear Regression RMSE')\n",
        "    ax1.plot(lead_time, resnet_rmse, 'o-', label='ResNet RMSE')\n",
        "    ax1.plot(lead_time, unet_rmse, 'o-', label='UNet RMSE')\n",
        "    ax1.plot(lead_time, vit_rmse, 'o-', label='ViT RMSE')\n",
        "    ax1.set_xlabel('Leadtime [hours]')\n",
        "    ax1.set_ylabel('RMSE')\n",
        "    ax1.set_title(name)\n",
        "    ax1.legend()\n",
        "    # Add grid\n",
        "    ax1.grid(True)\n",
        "    # Plot ACC on the second subplot\n",
        "    ax2.plot(lead_time, linreg_baseline_acc, 'o-', label='Linear Regression ACC')\n",
        "    ax2.plot(lead_time, resnet_acc, 'o-', label='ResNet ACC')\n",
        "    ax2.plot(lead_time, unet_acc, 'o-', label='UNet ACC')\n",
        "    ax2.plot(lead_time, vit_acc, 'o-', label='ViT ACC')\n",
        "    ax2.set_xlabel('Leadtime [hours]')\n",
        "    ax2.set_ylabel('ACC')\n",
        "    ax2.set_title(name)\n",
        "    ax2.legend()\n",
        "    # Add grid\n",
        "    ax2.grid(True)\n",
        "    # Adjust spacing and layout\n",
        "    plt.tight_layout()\n",
        "    # Add legend\n",
        "    plt.legend()\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def create_list(stats):\n",
        "    l1, l2, l3 = [], [], []\n",
        "    lists = [l1, l2, l3]\n",
        "    for i in range(len(lists)):  # Variable\n",
        "        for j in range(len(stats)):  # Net\n",
        "            lists[i].append(stats[j][i])\n",
        "    return lists\n",
        "\n",
        "\n",
        "def printProgressBar(iteration, total, prefix='', suffix='', decimals=1, length=100, fill='â–ˆ'):\n",
        "    \"\"\"\n",
        "    Call in a loop to create terminal progress bar\n",
        "    @params:\n",
        "        iteration   - Required  : current iteration (Int)\n",
        "        total       - Required  : total iterations (Int)\n",
        "        prefix      - Optional  : prefix string (Str)\n",
        "        suffix      - Optional  : suffix string (Str)\n",
        "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
        "        length      - Optional  : character length of bar (Int)\n",
        "        fill        - Optional  : bar fill character (Str)\n",
        "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
        "    \"\"\"\n",
        "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
        "    filledLength = int(length * iteration // total)\n",
        "    bar = fill * filledLength + '-' * (length - 1 - filledLength)\n",
        "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end=\" \")\n",
        "    # Print New Line on Complete\n",
        "    if iteration == total:\n",
        "        print()\n",
        "\n",
        "\n",
        "def printProgressAction(action, iteration):\n",
        "    print(f'\\r{action} {iteration}', end=\" \")\n",
        "\n",
        "\n",
        "class PeriodicPadding2D(nn.Module):\n",
        "    def __init__(self, pad_width):\n",
        "        super().__init__()\n",
        "        self.pad_width = pad_width\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.pad_width == 0:\n",
        "            return inputs\n",
        "        inputs_padded = torch.cat((inputs[:, :, :, -self.pad_width:],\n",
        "                                   inputs,\n",
        "                                   inputs[:, :, :, :self.pad_width],), dim=-1, ).to(device)\n",
        "        inputs_padded = nn.functional.pad(inputs_padded, (0, 0, self.pad_width, self.pad_width), ).to(device)\n",
        "        return inputs_padded\n",
        "\n",
        "\n",
        "def EarlyStopping(curr_monitor, old_monitor, count, patience=5, min_delta=0):\n",
        "    stop = False\n",
        "    if (curr_monitor - old_monitor) <= min_delta:\n",
        "        count += 1\n",
        "        if count > patience:\n",
        "            stop = True\n",
        "            return count, stop\n",
        "    count = 0\n",
        "    return count, stop\n",
        "\n",
        "\n",
        "def lr_schedulers(Net_optimizer):\n",
        "    Net_linearLR = optim.lr_scheduler.LinearLR(Net_optimizer, total_iters=5)\n",
        "    Net_cos_annLR = optim.lr_scheduler.CosineAnnealingLR(Net_optimizer, T_max=45, eta_min=3.75e-4)\n",
        "    return Net_linearLR, Net_cos_annLR\n",
        "\n",
        "\n",
        "def check(Net, data, six_hours_ago, twelve_hours_ago, target, constants, latitude_weights):\n",
        "    target = target[:, [4, 9, 33], :, :]\n",
        "    pred = Net(data, six_hours_ago, twelve_hours_ago, target, constants)\n",
        "    loss = loss_function(pred, target, latitude_weights)\n",
        "    return loss\n",
        "\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, lead_time_6h: list, lead_time_24h: list, lead_time_72h: list, lead_time_120h: list,\n",
        "                 lead_time_240h: list):\n",
        "        super(CustomDataset, self).__init__()\n",
        "        # Lead Time 6\n",
        "        self.data_lt_6 = lead_time_6h[0]\n",
        "        self.six_hours_ago_lt_6 = lead_time_6h[1]\n",
        "        self.twelve_hours_ago_lt_6 = lead_time_6h[2]\n",
        "        self.target_6 = lead_time_6h[3]\n",
        "        # Lead Time 24\n",
        "        self.data_lt_24 = lead_time_24h[0]\n",
        "        self.six_hours_ago_lt_24 = lead_time_24h[1]\n",
        "        self.twelve_hours_ago_lt_24 = lead_time_24h[2]\n",
        "        self.target_24 = lead_time_24h[3]\n",
        "        # Lead Time 72\n",
        "        self.data_lt_72 = lead_time_72h[0]\n",
        "        self.six_hours_ago_lt_72 = lead_time_72h[1]\n",
        "        self.twelve_hours_ago_lt_72 = lead_time_72h[2]\n",
        "        self.target_72 = lead_time_72h[3]\n",
        "        # Lead Time 120\n",
        "        self.data_lt_120 = lead_time_120h[0]\n",
        "        self.six_hours_ago_lt_120 = lead_time_120h[1]\n",
        "        self.twelve_hours_ago_lt_120 = lead_time_120h[2]\n",
        "        self.target_120 = lead_time_120h[3]\n",
        "        # Lead Time 240\n",
        "        self.data_lt_240 = lead_time_240h[0]\n",
        "        self.six_hours_ago_lt_240 = lead_time_240h[1]\n",
        "        self.twelve_hours_ago_lt_240 = lead_time_240h[2]\n",
        "        self.target_240 = lead_time_240h[3]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_lt_240)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        l_t_6 = [self.data_lt_6[idx], self.six_hours_ago_lt_6[idx], self.twelve_hours_ago_lt_6[idx],\n",
        "                 self.target_6[idx]]\n",
        "        l_t_24 = [self.data_lt_24[idx], self.six_hours_ago_lt_24[idx], self.twelve_hours_ago_lt_24[idx],\n",
        "                  self.target_24[idx]]\n",
        "        l_t_72 = [self.data_lt_72[idx], self.six_hours_ago_lt_72[idx], self.twelve_hours_ago_lt_72[idx],\n",
        "                  self.target_72[idx]]\n",
        "        l_t_120 = [self.data_lt_120[idx], self.six_hours_ago_lt_120[idx], self.twelve_hours_ago_lt_120[idx],\n",
        "                   self.target_120[idx]]\n",
        "        l_t_240 = [self.data_lt_240[idx], self.six_hours_ago_lt_240[idx], self.twelve_hours_ago_lt_240[idx],\n",
        "                   self.target_240[idx]]\n",
        "        return l_t_6, l_t_24, l_t_72, l_t_120, l_t_240\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.periodic_zeros_padding = PeriodicPadding2D(1)\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=0).to(device)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.3).to(device)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels).to(device)\n",
        "        self.dropout = nn.Dropout(0.1).to(device)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1).to(device)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels).to(device)\n",
        "        self.shortcut = nn.Identity().to(device)\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1)).to(device),\n",
        "                nn.BatchNorm2d(out_channels).to(device)\n",
        "            ).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.shortcut(x)\n",
        "        pad_x = self.periodic_zeros_padding(x)\n",
        "        x = self.conv1(pad_x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + residual\n",
        "        return x"
      ],
      "metadata": {
        "id": "6GaObA_T9ysD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Processing**\n",
        "'define_sets(task, val=0, test=0)' loads '.nc' files and turns them into lists; according to the values of 'task', 'val' and 'test' are defined the constant set, the training set or the validation/test set. The PreProcessing class initialises and uses the function that normalises network inputs."
      ],
      "metadata": {
        "id": "mGUZDKnoo1Sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreProcessing:\n",
        "    def __init__(self):\n",
        "        self.scaler = sklearn.preprocessing.StandardScaler()\n",
        "\n",
        "    def process(self, dataset):                                            # dataset has shape (batch_size, 141, 32, 64)\n",
        "        processed_data = []\n",
        "        for i in range(len(dataset)):                                      # len(dataset) == batch_size\n",
        "            processed_batch = []\n",
        "            for j in range(len(dataset[i])):                               # len(dataset[i]) == 141\n",
        "                processed_batch.append(self.scaler.fit_transform(X=dataset[i][j]))\n",
        "            processed_data.append(np.array(processed_batch, dtype=np.float32))\n",
        "        processed_dataset = torch.FloatTensor(np.array(processed_data, dtype=np.float32)).to(device)\n",
        "        return processed_dataset\n",
        "\n",
        "\n",
        "def define_sets(task, val=0, test=0):              # Loads .nc files and turns them into lists\n",
        "    # Constants\n",
        "    if task == 'const':\n",
        "        print(\"Constants Set Definition\")\n",
        "        lsm, orography, lat2d = [], [], []\n",
        "        nc = netCDF4.Dataset(f\"{path}/{static_variable}.nc\")\n",
        "        for i in range(0, 3):\n",
        "            data = nc[abbr[i]]\n",
        "            data_np = data[:]\n",
        "            if i == 0:\n",
        "                lsm.append(data_np)\n",
        "            if i == 1:\n",
        "                orography.append(data_np)\n",
        "            if i == 2:\n",
        "                lat2d.append(data_np)\n",
        "        constants_set = lsm + orography + lat2d\n",
        "        return constants_set\n",
        "    # Define train_set\n",
        "    if task == 'train':\n",
        "        print(\"Train Set Definition\")\n",
        "        train_tisr, train_t2m, train_u10, train_v10 = [], [], [], []\n",
        "        train_z, train_u, train_v, train_t, train_q, train_r, = [], [], [], [], [], []\n",
        "        j, l = 0, (max_bound_year_train - low_bound_year_train)\n",
        "        printProgressBar(j, l, prefix='Progress:', suffix='Complete', length=50)\n",
        "        for year in range(low_bound_year_train, max_bound_year_train):\n",
        "            for i in range(3, len(abbr)):\n",
        "                data, data_np = [], []\n",
        "                if 2 < i < 7:\n",
        "                    nc = netCDF4.Dataset(f\"{path}/{single_folder[i - 3]}/{single_variable[i - 3]}{year}{resolution}.nc\")\n",
        "                    data = nc[abbr[i]]\n",
        "                    data_np = data[:]\n",
        "                    # Remove the last 24 hours if this year has 366 days\n",
        "                    if data_np.shape[0] == 8784:\n",
        "                        data_np = data_np[:8760]\n",
        "                if i == 3:\n",
        "                    train_tisr.append(data_np)\n",
        "                if i == 4:\n",
        "                    train_t2m.append(data_np)\n",
        "                if i == 5:\n",
        "                    train_u10.append(data_np)\n",
        "                if i == 6:\n",
        "                    train_v10.append(data_np)\n",
        "\n",
        "                level = []\n",
        "                if 6 < i < 13:\n",
        "                    nc = netCDF4.Dataset(f\"{path}/{atmospheric_folder[i - 7]}/\"\n",
        "                                         f\"{atmospheric_variable[i - 7]}{year}{resolution}.nc\")\n",
        "                    data = nc[abbr[i]]\n",
        "                    data_np = data[:]\n",
        "                    # Remove the last 24 hours if this year has 366 days\n",
        "                    if data_np.shape[0] == 8784:\n",
        "                        data_np = data_np[:8760]\n",
        "                    level = []\n",
        "                    for lev in lev_indexes:\n",
        "                        level.append(data_np[:, lev])\n",
        "                if i == 7:\n",
        "                    train_z.append(level)\n",
        "                if i == 8:\n",
        "                    train_u.append(level)\n",
        "                if i == 9:\n",
        "                    train_v.append(level)\n",
        "                if i == 10:\n",
        "                    train_t.append(level)\n",
        "                if i == 11:\n",
        "                    train_q.append(level)\n",
        "                if i == 12:\n",
        "                    train_r.append(level)\n",
        "            j += 1\n",
        "            printProgressBar(j, l, prefix='Progress:', suffix='Complete', length=50)\n",
        "        train_list = []\n",
        "        for i in range(max_year_train + 1):\n",
        "            for j in range(8760):\n",
        "                tr_list = []\n",
        "                tr_list.append(train_tisr[i][j]), tr_list.append(train_t2m[i][j]), \\\n",
        "                tr_list.append(train_u10[i][j]), tr_list.append(train_v10[i][j])\n",
        "                for lev in range(len(levels)):\n",
        "                    tr_list.append(train_z[i][lev][j]), tr_list.append(train_u[i][lev][j]), \\\n",
        "                    tr_list.append(train_v[i][lev][j]), tr_list.append(train_t[i][lev][j]), \\\n",
        "                    tr_list.append(train_q[i][lev][j]), tr_list.append(train_r[i][lev][j])\n",
        "                train_list.append(np.array(tr_list))\n",
        "                #train_list.append(tr_list)\n",
        "        train_set = train_list\n",
        "        return train_set\n",
        "\n",
        "    # Define validation_set and test_set\n",
        "    if task == 'val_test':\n",
        "        print(\"Validation and Test Sets Definition\")\n",
        "        val_tisr, val_t2m, val_u10, val_v10 = [], [], [], []\n",
        "        val_z, val_u, val_v, val_t, val_q, val_r, = [], [], [], [], [], []\n",
        "        test_tisr, test_t2m, test_u10, test_v10 = [], [], [], []\n",
        "        test_z, test_u, test_v, test_t, test_q, test_r, = [], [], [], [], [], []\n",
        "        j, l = 0, (max_bound_year_val_test - low_bound_year_val_test)\n",
        "        printProgressBar(j, l, prefix='Progress:', suffix='Complete', length=50)\n",
        "        for year in range(low_bound_year_val_test, max_bound_year_val_test):\n",
        "            for i in range(3, len(abbr)):\n",
        "                data, data_np = [], []\n",
        "                if 2 < i < 7:\n",
        "                    nc = netCDF4.Dataset(f\"{path}/{single_folder[i - 3]}/{single_variable[i - 3]}{year}{resolution}.nc\")\n",
        "                    data = nc[abbr[i]]\n",
        "                    data_np = data[:]\n",
        "                    # Remove the last 24 hours if this year has 366 days\n",
        "                    if data_np.shape[0] == 8784:\n",
        "                        data_np = data_np[:8760]\n",
        "                if i == 3:\n",
        "                  if val:\n",
        "                    val_tisr.append(data_np[0:4380])\n",
        "                  if test:\n",
        "                    test_tisr.append(data_np[4380:8760])\n",
        "                if i == 4:\n",
        "                  if val:\n",
        "                    val_t2m.append(data_np[0:4380])\n",
        "                  if test:\n",
        "                    test_t2m.append(data_np[4380:8760])\n",
        "                if i == 5:\n",
        "                  if val:\n",
        "                    val_u10.append(data_np[0:4380])\n",
        "                  if test:\n",
        "                    test_u10.append(data_np[4380:8760])\n",
        "                if i == 6:\n",
        "                  if val:\n",
        "                    val_v10.append(data_np[0:4380])\n",
        "                  if test:\n",
        "                    test_v10.append(data_np[4380:8760])\n",
        "\n",
        "                level_val, level_test = [], []\n",
        "                if 6 < i < 13:\n",
        "                    nc = netCDF4.Dataset(f\"{path}/{atmospheric_folder[i - 7]}/\"\n",
        "                                         f\"{atmospheric_variable[i - 7]}{year}{resolution}.nc\")\n",
        "                    data = nc[abbr[i]]\n",
        "                    data_np = data[0:4380]\n",
        "                    # Remove the last 24 hours if this year has 366 days\n",
        "                    if data_np.shape[0] == 8784:\n",
        "                        data_np = data_np[:8760]\n",
        "                    level_val, level_test = [], []\n",
        "                    for lev in lev_indexes:\n",
        "                      if val:\n",
        "                        level_val.append(data_np[0:4380, lev])\n",
        "                      if test:\n",
        "                        level_test.append(data_np[4380:8760, lev])\n",
        "                if i == 7:\n",
        "                  if val:\n",
        "                    val_z.append(level_val)\n",
        "                  if test:\n",
        "                    test_z.append(level_test)\n",
        "                if i == 8:\n",
        "                  if val:\n",
        "                    val_u.append(level_val)\n",
        "                  if test:\n",
        "                    test_u.append(level_test)\n",
        "                if i == 9:\n",
        "                  if val:\n",
        "                    val_v.append(level_val)\n",
        "                  if test:\n",
        "                    test_v.append(level_test)\n",
        "                if i == 10:\n",
        "                  if val:\n",
        "                    val_t.append(level_val)\n",
        "                  if test:\n",
        "                    test_t.append(level_test)\n",
        "                if i == 11:\n",
        "                  if val:\n",
        "                    val_q.append(level_val)\n",
        "                  if test:\n",
        "                    test_q.append(level_test)\n",
        "                if i == 12:\n",
        "                  if val:\n",
        "                    val_r.append(level_val)\n",
        "                  if test:\n",
        "                    test_r.append(level_test)\n",
        "            j += 1\n",
        "            printProgressBar(j, l, prefix='Progress:', suffix='Complete', length=50)\n",
        "        validation_list, test_list = [], []\n",
        "        for i in range(max_year_val + 1):\n",
        "            for j in range(4380):\n",
        "                val_list, tst_list = [], []\n",
        "                if val:\n",
        "                  val_list.append(val_tisr[i][j]), val_list.append(val_t2m[i][j]), \\\n",
        "                  val_list.append(val_u10[i][j]), val_list.append(val_v10[i][j])\n",
        "                if test:\n",
        "                  tst_list.append(test_tisr[i][j]), tst_list.append(test_t2m[i][j]), \\\n",
        "                  tst_list.append(test_u10[i][j]), tst_list.append(test_v10[i][j])\n",
        "                for lev in range(len(levels)):\n",
        "                    if val:\n",
        "                      val_list.append(val_z[i][lev][j]), val_list.append(val_u[i][lev][j]), \\\n",
        "                      val_list.append(val_v[i][lev][j]), val_list.append(val_t[i][lev][j]), \\\n",
        "                      val_list.append(val_q[i][lev][j]), val_list.append(val_r[i][lev][j])\n",
        "                    if test:\n",
        "                      tst_list.append(test_z[i][lev][j]), tst_list.append(test_u[i][lev][j]), \\\n",
        "                      tst_list.append(test_v[i][lev][j]), tst_list.append(test_t[i][lev][j]), \\\n",
        "                      tst_list.append(test_q[i][lev][j]), tst_list.append(test_r[i][lev][j])\n",
        "                if val:\n",
        "                    validation_list.append(np.array(val_list))\n",
        "                    #validation_list.append(val_list)\n",
        "                if test:\n",
        "                    test_list.append(np.array(tst_list))\n",
        "                    #test_list.append(tst_list)\n",
        "        validation_set = validation_list\n",
        "        test_set = test_list\n",
        "        return validation_set, test_set"
      ],
      "metadata": {
        "id": "zBA97JMH7OI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "constants_set = define_sets('const')"
      ],
      "metadata": {
        "id": "5Esp5yPwBYDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = define_sets('train')"
      ],
      "metadata": {
        "id": "BpybAY6fBacj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_set, test_set = define_sets('val_test', val=1, test=1)"
      ],
      "metadata": {
        "id": "ww6ODYN3o0Wu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6433a9-cbdd-4337-d8e2-02b0469ba3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation and Test Sets Definition\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Neural Networks**\n",
        "Here, the three neural networks, the baseline and the generic class â€˜Networkâ€™ are defined; the latter class creates a generic network with all the methods required for training."
      ],
      "metadata": {
        "id": "OrkdK1-9tmhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet"
      ],
      "metadata": {
        "id": "zsdwcD-kttwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, processor, hidden_channels=128, num_blocks=28):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.periodic_zeros_padding = PeriodicPadding2D(3)\n",
        "        self.image_projection = nn.Conv2d(in_channels, hidden_channels, kernel_size=7, stride=1, padding=0).to(device)\n",
        "        self.res_net_blocks = make_layer(ResidualBlock, hidden_channels, hidden_channels, num_blocks=num_blocks, change=1)\n",
        "        self.norm = nn.BatchNorm2d(hidden_channels).to(device)\n",
        "        self.out = nn.Conv2d(hidden_channels, out_channels, kernel_size=7, stride=1, padding=3).to(device)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.3).to(device)\n",
        "        self.processor = processor\n",
        "        self.set_climatology = []\n",
        "\n",
        "    def __call__(self, data, six_hours_ago, twelve_hours_ago, target, constants):\n",
        "        current_data = np.concatenate((constants, data), axis=1)\n",
        "        pred = self.forward(np.concatenate((current_data, six_hours_ago, twelve_hours_ago), axis=1))\n",
        "        self.set_climatology.append(target)\n",
        "        return pred\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.processor.process(x)\n",
        "        x = self.image_projection(self.periodic_zeros_padding(x))\n",
        "        x = self.res_net_blocks(x)\n",
        "        x = self.out(self.leaky_relu(self.norm(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "06ONJbqdtx71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNet"
      ],
      "metadata": {
        "id": "9c7tnG15fEiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, processor, hidden_channels=64, channel_multiplications=(1, 2, 2),\n",
        "                 blocks=2, use_attention=False):\n",
        "        super(UNet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.processor = processor\n",
        "        self.set_climatology = []\n",
        "        self.periodic_zeros_padding = PeriodicPadding2D(3)\n",
        "        self.conv1 = nn.Conv2d(self.in_channels, self.hidden_channels, kernel_size=7, stride=1, padding=0).to(device)\n",
        "        out_channels = in_channels = self.hidden_channels\n",
        "        self.n_resolutions = len(channel_multiplications)\n",
        "        self.blocks = blocks\n",
        "        # Downward path\n",
        "        self.down_blocks = []\n",
        "        for i in range(self.n_resolutions):     # 3\n",
        "            out_channels = in_channels * channel_multiplications[i]\n",
        "            for _ in range(self.blocks):     # 2\n",
        "                self.down_blocks.append(ResidualBlock(in_channels, out_channels))\n",
        "                in_channels = out_channels\n",
        "            # Down sample at all resolutions except the last\n",
        "            # Scale down the feature map by 0.5 times\n",
        "            if i < self.n_resolutions - 1:\n",
        "                self.down_blocks.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=1).to(device))\n",
        "        self.down_blocks = nn.ModuleList(self.down_blocks).to(device)\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            ResidualBlock(out_channels, out_channels),\n",
        "            ResidualBlock(out_channels, out_channels)\n",
        "        ).to(device)\n",
        "        # Upward path\n",
        "        self.up_blocks = []\n",
        "        for i in reversed(range(self.n_resolutions)):       # 3\n",
        "            out_channels = in_channels\n",
        "            for _ in range(self.blocks):     # 2\n",
        "                self.up_blocks.append(ResidualBlock(in_channels+out_channels, out_channels))\n",
        "            out_channels = in_channels // channel_multiplications[i]\n",
        "            self.up_blocks.append(ResidualBlock(in_channels + out_channels, out_channels))\n",
        "            in_channels = out_channels\n",
        "            # Up sample at all resolutions except last\n",
        "            if i > 0:\n",
        "                self.up_blocks.append(nn.ConvTranspose2d(in_channels, in_channels, kernel_size=4, stride=2, padding=1).to(device))\n",
        "        self.up_blocks = nn.ModuleList(self.up_blocks).to(device)\n",
        "        self.norm = nn.BatchNorm2d(self.hidden_channels).to(device)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.3).to(device)\n",
        "        self.out = nn.Conv2d(in_channels, self.out_channels, kernel_size=7, padding=0).to(device)\n",
        "\n",
        "    def __call__(self, data, six_hours_ago, twelve_hours_ago, target, constants):\n",
        "        current_data = np.concatenate((constants, data), axis=1)\n",
        "        pred = self.forward(np.concatenate((current_data, six_hours_ago, twelve_hours_ago), axis=1))\n",
        "        self.set_climatology.append(target)\n",
        "        return pred\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.processor.process(x)\n",
        "        x = self.conv1(self.periodic_zeros_padding(x))\n",
        "        skips = [x]\n",
        "        # Downward path\n",
        "        for down_block in self.down_blocks:\n",
        "            x = down_block(x)\n",
        "            skips.append(x)\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "        # Upward path\n",
        "        for up_block in self.up_blocks:\n",
        "            if isinstance(up_block, nn.ConvTranspose2d):\n",
        "                x = up_block(x)\n",
        "            else:\n",
        "                skip_connection = skips.pop()\n",
        "                x = torch.cat((x, skip_connection), dim=1).to(device)\n",
        "                x = up_block(x)\n",
        "        x = self.periodic_zeros_padding(x)\n",
        "        x = self.out(self.leaky_relu(self.norm(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "wIYUJN1tfFl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT"
      ],
      "metadata": {
        "id": "MKb7gD0qfKS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, img_size, processor, patch_size=2, embedding_dim=128, depth=8,\n",
        "                 num_heads=4, hidden_dimension=128, mlp_ratio=4, prediction_depth=2, drop_path_rate=0.1,\n",
        "                 dropout_rate=0.1, learn_positional_embedding=False):\n",
        "        super(ViT, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.patch_size = patch_size\n",
        "        self.processor = processor\n",
        "        self.set_climatology = []\n",
        "        # Patch Embedding\n",
        "        self.patch_embedding = PatchEmbed(self.img_size, patch_size, self.in_channels, embedding_dim).to(device)\n",
        "        self.num_patches = self.patch_embedding.num_patches\n",
        "        # Position Embedding\n",
        "        self.pos_embed = nn.Parameter(self.trigonometric_pos(embedding_dim),\n",
        "                                      requires_grad=learn_positional_embedding).to(device)\n",
        "        self.pos_drop = nn.Dropout(dropout_rate).to(device)\n",
        "        self.vit_block = []\n",
        "        for _ in range(depth):\n",
        "            self.vit_block.append(ViTBlock(embedding_dim, num_heads, mlp_ratio, dropout_rate, drop_path_rate))\n",
        "        self.vit_block = nn.ModuleList(self.vit_block).to(device)\n",
        "        # Prediction head (MLP Head)\n",
        "        prediction_layers = []\n",
        "        for _ in range(prediction_depth):\n",
        "            prediction_layers.append(nn.Linear(hidden_dimension, hidden_dimension).to(device))\n",
        "            prediction_layers.append(nn.LeakyReLU().to(device))\n",
        "        prediction_layers.append(nn.Linear(hidden_dimension, out_channels * patch_size ** 2).to(device))\n",
        "        self.prediction_head = nn.Sequential(*prediction_layers).to(device)\n",
        "\n",
        "    def __call__(self, data, six_hours_ago, twelve_hours_ago, target, constants):\n",
        "        current_data = np.concatenate((constants, data), axis=1)\n",
        "        pred = self.forward(np.concatenate((current_data, six_hours_ago, twelve_hours_ago), axis=1))\n",
        "        self.set_climatology.append(target)\n",
        "        return pred\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.processor.process(x)\n",
        "        # Patch embedding\n",
        "        x = self.patch_embedding(x)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        # Encoder\n",
        "        for vit_block in self.vit_block:\n",
        "            x = vit_block.forward(x)\n",
        "        # Prediction head\n",
        "        x = self.prediction_head(x)\n",
        "        x = return_to_image(x, self.patch_size, self.out_channels, self.img_size)\n",
        "        return x\n",
        "\n",
        "    def trigonometric_pos(self, embedding_dim, n=10000):\n",
        "        pos_embed = torch.zeros(1, self.num_patches, embedding_dim).to(device)\n",
        "        for i in range(1):\n",
        "            for j in range(self.num_patches):\n",
        "                for k in range(int(embedding_dim / 2)):\n",
        "                    denominator = np.power(n, 2 * i / embedding_dim)\n",
        "                    pos_embed[i, k, 2 * i] = np.sin(k / denominator)\n",
        "                    pos_embed[i, k, 2 * i + 1] = np.cos(k / denominator)\n",
        "        return pos_embed\n",
        "\n",
        "\n",
        "class ViTBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, mlp_ratio, dropout_rate, drop_path_rate):\n",
        "        super(ViTBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim).to(device)\n",
        "\n",
        "        self.query = nn.Linear(embedding_dim, embedding_dim).to(device)\n",
        "        self.key = nn.Linear(embedding_dim, embedding_dim).to(device)\n",
        "        self.value = nn.Linear(embedding_dim, embedding_dim).to(device)\n",
        "\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate).to(device)\n",
        "        if drop_path_rate > 0:\n",
        "            self.drop_path1 = DropPath(drop_path_rate)\n",
        "        else:\n",
        "            self.drop_path1 = nn.Identity().to(device)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim).to(device)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, int(embedding_dim * mlp_ratio)).to(device),\n",
        "            nn.LeakyReLU().to(device),\n",
        "            nn.Dropout(dropout_rate).to(device),\n",
        "            nn.Linear(int(embedding_dim * mlp_ratio), embedding_dim).to(device),\n",
        "            nn.LeakyReLU().to(device),\n",
        "            nn.Dropout(dropout_rate).to(device)\n",
        "        ).to(device)\n",
        "        if drop_path_rate > 0:\n",
        "            self.drop_path2 = DropPath(drop_path_rate)\n",
        "        else:\n",
        "            self.drop_path2 = nn.Identity().to(device)\n",
        "\n",
        "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
        "        x = self.norm1(x)\n",
        "        y = self.mha(query=self.query(x), key=self.key(x), value=self.value(x))\n",
        "        x = x + self.drop_path1(y[0])\n",
        "        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "FVGT6Y0YfJTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression (Baseline)"
      ],
      "metadata": {
        "id": "BNAOIEKgmAoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        height = x.shape[3]\n",
        "        width = x.shape[4]\n",
        "        x = x.flatten(1)\n",
        "        x = self.linear(x)\n",
        "        x = x.reshape(batch_size, -1, height, width)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VbAmObk3mABY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generic class 'Network'"
      ],
      "metadata": {
        "id": "i_AjPeErfRX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self, Net, lat_weights, name, lead_time):\n",
        "        self.Net = Net\n",
        "        self.name = name\n",
        "        self.lead_time = lead_time\n",
        "        self.Net_optimizer = optim.AdamW(self.Net.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "        self.Net_linearLR, self.Net_cos_annLR = lr_schedulers(self.Net_optimizer)\n",
        "        self.low_year_train, self.max_year_train = low_year_train, max_year_train\n",
        "        self.low_hour_train, self.max_hour_train = low_hour_train, max_hour_train\n",
        "        self.low_year_val, self.max_year_val = low_year_val, max_year_val\n",
        "        self.low_hour_val, self.max_hour_val = low_hour_val, max_hour_val\n",
        "        self.lat_weights = lat_weights\n",
        "        self.done = False\n",
        "        self.count = 0\n",
        "        self.loss = None\n",
        "        self.validation_losses = []\n",
        "        self.previous_validation_losses = self.validation_losses\n",
        "        self.pred, self.targ = [], []\n",
        "\n",
        "    def pre_steps(self):\n",
        "        if not self.done:\n",
        "            self.count = 0\n",
        "            self.loss = None\n",
        "            self.validation_losses = []\n",
        "            self.previous_validation_losses = self.validation_losses\n",
        "\n",
        "    def train_step(self, input_list, constants):\n",
        "        # input_list = [data, six_hours_ago, twelve_hours_ago, target]\n",
        "        if not self.done:\n",
        "            self.loss = check(self.Net, input_list[0], input_list[1], input_list[2], input_list[3], constants,\n",
        "                              self.lat_weights)\n",
        "\n",
        "    def val_step(self, input_list, constants):\n",
        "        # input_list = [data, six_hours_ago, twelve_hours_ago, target]\n",
        "        if not self.done:\n",
        "            loss = check(self.Net, input_list[0], input_list[1], input_list[2], input_list[3], constants,\n",
        "                         self.lat_weights)\n",
        "            self.validation_losses.append(loss.item())\n",
        "\n",
        "    def post_steps(self, epoch):\n",
        "        # Optimization\n",
        "        if not self.done:\n",
        "            loss = self.loss\n",
        "            self.Net_optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.Net_optimizer.step()\n",
        "            # Learning Rate update\n",
        "            # Linear warmup schedule if 'epoch < 5' - Cosine-annealing warmup schedule 'else'\n",
        "            if epoch < 5:\n",
        "                self.Net_linearLR.step()\n",
        "                self.Net_optimizer.defaults['lr'] = self.Net_linearLR.get_last_lr()\n",
        "            else:\n",
        "                self.Net_cos_annLR.step()\n",
        "                self.Net_optimizer.defaults['lr'] = self.Net_cos_annLR.get_last_lr()\n",
        "\n",
        "    def stopping(self):\n",
        "        if self.previous_validation_losses is None:\n",
        "            self.count, self.done = 0, False\n",
        "        else:\n",
        "            self.count, self.done = EarlyStopping(statistics.mean(self.validation_losses),\n",
        "                                                  statistics.mean(self.previous_validation_losses), self.count)\n",
        "        if self.done:\n",
        "            print(\"{}_{} is done\".format(self.name, self.lead_time))\n",
        "\n",
        "    def eval_step(self, data, six_hours_ago, twelve_hours_ago, target, constants):\n",
        "        p = self.Net.Net(data, six_hours_ago, twelve_hours_ago, target, constants)\n",
        "        self.pred.append(p.detach().numpy()), self.targ.append(target.detach().numpy())"
      ],
      "metadata": {
        "id": "ZUJxq1TRfrsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "Contains the loss function and metrics used for evaluation."
      ],
      "metadata": {
        "id": "OG4ITjP07j0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def latitude_weighting_function(latitude_coordinates):\n",
        "    # latitude_coordinates is an array of 'H' elements\n",
        "    num = np.cos(np.deg2rad(latitude_coordinates))\n",
        "    den = sum(num) / len(num)\n",
        "    latitude_weights = num / den\n",
        "    latitude_weights = torch.from_numpy(latitude_weights).view(1, 1, -1, 1).to(device)\n",
        "    return latitude_weights.cpu()\n",
        "\n",
        "\n",
        "def loss_function(prediction, target, latitude_weights):                            # LW_MSE\n",
        "    error = latitude_weights * torch.square(prediction - target).to(device)\n",
        "    result = torch.mean(torch.mean(error, dim=[0, 2, 3]).to(device), dim=0).to(device)\n",
        "    return result\n",
        "\n",
        "\n",
        "def LW_RMSE(prediction, target, latitude_weights):\n",
        "    diff = [x - y for x, y in zip(prediction, target)]\n",
        "    error = latitude_weights * np.square(diff)\n",
        "    channel_rmse = error.mean([3, 4]).sqrt().mean(1)\n",
        "    result = channel_rmse.mean(0)\n",
        "    return result.cpu().numpy()\n",
        "\n",
        "\n",
        "def LW_ACC(prediction, target, latitude_weights, climatology):\n",
        "    climatology = np.asarray(climatology).mean(0)\n",
        "    prediction = prediction - climatology\n",
        "    target = target - climatology\n",
        "    channel_acc = []\n",
        "    for i in range(prediction.shape[1]):\n",
        "        pred_prime = prediction[:, i] - prediction[:, i].mean()\n",
        "        target_prime = target[:, i] - target[:, i].mean()\n",
        "        numer = (latitude_weights * pred_prime * target_prime).sum()\n",
        "        denom_1 = (latitude_weights * np.square(pred_prime)).sum()\n",
        "        denom_2 = (latitude_weights * np.square(target_prime)).sum()\n",
        "        channel_acc.append(numer / np.sqrt(denom_1 * denom_2))\n",
        "    channel_acc = torch.stack(channel_acc).to(device)\n",
        "    result = channel_acc\n",
        "    return result.cpu().numpy()\n",
        "\n",
        "\n",
        "def compute_eval(prediction, target, latitude_weights, set_climatology):\n",
        "    rmse = LW_RMSE(prediction, target, latitude_weights)\n",
        "    acc = LW_ACC(prediction, target, latitude_weights, set_climatology)\n",
        "    return rmse, acc"
      ],
      "metadata": {
        "id": "JJhjLSUO7sfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forecasting\n",
        "This class manages the 15 networks and the baseline and their training. Networks are of three types: 5 ResNets, 5 UNets and 5 ViTs; their high number is due to the fact that we are considering 5 lead times for forecasting: [6, 24, 72, 120, 240] hours. The baseline is a Linear Regression model."
      ],
      "metadata": {
        "id": "58m8wDxLfom1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Forecasting(nn.Module):\n",
        "    def __init__(self, constants_set, train_data, validation_data, batch_size=128,\n",
        "                 res_params=list[128, 28], u_params=list[64, 2], vit_params=list[8, 4, 2, 128]):\n",
        "        super(Forecasting, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_channels = 141\n",
        "        self.out_channels = 3\n",
        "        self.batch_size = None\n",
        "        self.img_size = (self.height, self.width) = (32, 64)\n",
        "        self.val_dim = len(validation_data)\n",
        "        self.processor = PreProcessing()\n",
        "        self.latitude_coordinates = latitude_coordinates\n",
        "        self.latitude_weights = latitude_weighting_function(self.latitude_coordinates)\n",
        "        self.constants = constants_set\n",
        "        self.validation_data = validation_data\n",
        "        train_years = max_year_train + 1 - low_year_train\n",
        "        train_6 = [train_data[12:((8760*train_years) - 6)], train_data[6:((8760*train_years) - 12)],\n",
        "                   train_data[0:((8760*train_years) - 18)], train_data[18:(8760*train_years)]]\n",
        "        train_24 = [train_data[12:((8760*train_years) - 24)], train_data[6:((8760*train_years) - 30)],\n",
        "                    train_data[0:((8760*train_years) - 36)], train_data[36:(8760*train_years)]]\n",
        "        train_72 = [train_data[12:((8760*train_years) - 72)], train_data[6:((8760*train_years) - 78)],\n",
        "                    train_data[0:((8760*train_years) - 84)], train_data[84:(8760*train_years)]]\n",
        "        train_120 = [train_data[12:((8760*train_years) - 120)], train_data[6:((8760*train_years) - 126)],\n",
        "                     train_data[0:((8760*train_years) - 132)], train_data[132:(8760*train_years)]]\n",
        "        train_240 = [train_data[12:((8760*train_years) - 240)], train_data[6:((8760*train_years) - 246)],\n",
        "                     train_data[0:((8760*train_years) - 252)], train_data[252:(8760*train_years)]]\n",
        "        train_set = CustomDataset(train_6, train_24, train_72, train_120, train_240)\n",
        "        self.train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "        val_6 = [validation_data[12:4374], validation_data[6:4368], validation_data[0:4362], validation_data[18:4380]]\n",
        "        val_24 = [validation_data[12:4356], validation_data[6:4350], validation_data[0:4344], validation_data[36:4380]]\n",
        "        val_72 = [validation_data[12:4308], validation_data[6:4302], validation_data[0:4296], validation_data[84:4380]]\n",
        "        val_120 = [validation_data[12:4260], validation_data[6:4254], validation_data[0:4248],\n",
        "                   validation_data[132:4380]]\n",
        "        val_240 = [validation_data[12:4140], validation_data[6:4134], validation_data[0:4128],\n",
        "                   validation_data[252:4380]]\n",
        "        validation_set = CustomDataset(val_6, val_24, val_72, val_120, val_240)\n",
        "        self.validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Baselines\n",
        "        self.LinReg_Baseline_6 = Network(\n",
        "            LinearRegression(self.num_channels,\n",
        "                             self.out_channels),\n",
        "            self.latitude_weights,\n",
        "            'LinearRegression',\n",
        "            6)\n",
        "        self.LinReg_Baseline_24 = Network(\n",
        "            LinearRegression(self.num_channels,\n",
        "                             self.out_channels),\n",
        "            self.latitude_weights,\n",
        "            'LinearRegression',\n",
        "            24)\n",
        "        self.LinReg_Baseline_72 = Network(\n",
        "            LinearRegression(self.num_channels,\n",
        "                             self.out_channels),\n",
        "            self.latitude_weights,\n",
        "            'LinearRegression',\n",
        "            72)\n",
        "        self.LinReg_Baseline_120 = Network(\n",
        "            LinearRegression(self.num_channels,\n",
        "                             self.out_channels),\n",
        "            self.latitude_weights,\n",
        "            'LinearRegression',\n",
        "            120)\n",
        "        self.LinReg_Baseline_240 = Network(\n",
        "            LinearRegression(self.num_channels,\n",
        "                             self.out_channels),\n",
        "            self.latitude_weights,\n",
        "            'LinearRegression',\n",
        "            240)\n",
        "        # ResNets\n",
        "        self.ResNet_6 = Network(\n",
        "            ResNet(self.num_channels,\n",
        "                   self.out_channels,\n",
        "                   self.processor,\n",
        "                   hidden_channels=res_params[0],\n",
        "                   num_blocks=res_params[1]),\n",
        "            self.latitude_weights,\n",
        "            'ResNet',\n",
        "            6)\n",
        "        self.ResNet_24 = Network(\n",
        "            ResNet(self.num_channels,\n",
        "                   self.out_channels,\n",
        "                   self.processor,\n",
        "                   hidden_channels=res_params[0],\n",
        "                   num_blocks=res_params[1]),\n",
        "            self.latitude_weights,\n",
        "            'ResNet',\n",
        "            24)\n",
        "        self.ResNet_72 = Network(\n",
        "            ResNet(self.num_channels,\n",
        "                   self.out_channels,\n",
        "                   self.processor,\n",
        "                   hidden_channels=res_params[0],\n",
        "                   num_blocks=res_params[1]),\n",
        "            self.latitude_weights,\n",
        "            'ResNet',\n",
        "            72)\n",
        "        self.ResNet_120 = Network(\n",
        "            ResNet(self.num_channels,\n",
        "                   self.out_channels,\n",
        "                   self.processor,\n",
        "                   hidden_channels=res_params[0],\n",
        "                   num_blocks=res_params[1]),\n",
        "            self.latitude_weights,\n",
        "            'ResNet',\n",
        "            120)\n",
        "        self.ResNet_240 = Network(\n",
        "            ResNet(self.num_channels,\n",
        "                   self.out_channels,\n",
        "                   self.processor,\n",
        "                   hidden_channels=res_params[0],\n",
        "                   num_blocks=res_params[1]),\n",
        "            self.latitude_weights,\n",
        "            'ResNet',\n",
        "            240)\n",
        "        # UNets\n",
        "        self.UNet_6 = Network(\n",
        "            UNet(self.num_channels,\n",
        "                 self.out_channels,\n",
        "                 self.processor,\n",
        "                 hidden_channels=u_params[0],\n",
        "                 blocks=u_params[1]),\n",
        "            self.latitude_weights,\n",
        "            'UNet',\n",
        "            6)\n",
        "        self.UNet_24 = Network(\n",
        "            UNet(self.num_channels,\n",
        "                 self.out_channels,\n",
        "                 self.processor,\n",
        "                 hidden_channels=u_params[0],\n",
        "                 blocks=u_params[1]),\n",
        "            self.latitude_weights,\n",
        "            'UNet',\n",
        "            24)\n",
        "        self.UNet_72 = Network(\n",
        "            UNet(self.num_channels,\n",
        "                 self.out_channels,\n",
        "                 self.processor,\n",
        "                 hidden_channels=u_params[0],\n",
        "                 blocks=u_params[1]),\n",
        "            self.latitude_weights,\n",
        "            'UNet',\n",
        "            72)\n",
        "        self.UNet_120 = Network(\n",
        "            UNet(self.num_channels,\n",
        "                 self.out_channels,\n",
        "                 self.processor,\n",
        "                 hidden_channels=u_params[0],\n",
        "                 blocks=u_params[1]),\n",
        "            self.latitude_weights,\n",
        "            'UNet',\n",
        "            120)\n",
        "        self.UNet_240 = Network(\n",
        "            UNet(self.num_channels,\n",
        "                 self.out_channels,\n",
        "                 self.processor,\n",
        "                 hidden_channels=u_params[0],\n",
        "                 blocks=u_params[1]),\n",
        "            self.latitude_weights,\n",
        "            'UNet',\n",
        "            240)\n",
        "        # ViTs\n",
        "        self.ViT_6 = Network(\n",
        "            ViT(self.num_channels,\n",
        "                self.out_channels,\n",
        "                self.img_size,\n",
        "                self.processor,\n",
        "                embedding_dim=vit_params[3],\n",
        "                depth=vit_params[0],\n",
        "                num_heads=vit_params[1],\n",
        "                prediction_depth=vit_params[2],\n",
        "                hidden_dimension=vit_params[3]),\n",
        "            self.latitude_weights,\n",
        "            'ViT', 6)\n",
        "        self.ViT_24 = Network(\n",
        "            ViT(self.num_channels,\n",
        "                self.out_channels,\n",
        "                self.img_size,\n",
        "                self.processor,\n",
        "                embedding_dim=vit_params[3],\n",
        "                depth=vit_params[0],\n",
        "                num_heads=vit_params[1],\n",
        "                prediction_depth=vit_params[2],\n",
        "                hidden_dimension=vit_params[3]),\n",
        "            self.latitude_weights,\n",
        "            'ViT', 24)\n",
        "        self.ViT_72 = Network(\n",
        "            ViT(self.num_channels,\n",
        "                self.out_channels,\n",
        "                self.img_size,\n",
        "                self.processor,\n",
        "                embedding_dim=vit_params[3],\n",
        "                depth=vit_params[0],\n",
        "                num_heads=vit_params[1],\n",
        "                prediction_depth=vit_params[2],\n",
        "                hidden_dimension=vit_params[3]),\n",
        "            self.latitude_weights,\n",
        "            'ViT', 72)\n",
        "        self.ViT_120 = Network(\n",
        "            ViT(self.num_channels,\n",
        "                self.out_channels,\n",
        "                self.img_size,\n",
        "                self.processor,\n",
        "                embedding_dim=vit_params[3],\n",
        "                depth=vit_params[0],\n",
        "                num_heads=vit_params[1],\n",
        "                prediction_depth=vit_params[2],\n",
        "                hidden_dimension=vit_params[3]),\n",
        "            self.latitude_weights,\n",
        "            'ViT', 120)\n",
        "        self.ViT_240 = Network(\n",
        "            ViT(self.num_channels,\n",
        "                self.out_channels,\n",
        "                self.img_size,\n",
        "                self.processor,\n",
        "                embedding_dim=vit_params[3],\n",
        "                depth=vit_params[0],\n",
        "                num_heads=vit_params[1],\n",
        "                prediction_depth=vit_params[2],\n",
        "                hidden_dimension=vit_params[3]),\n",
        "            self.latitude_weights,\n",
        "            'ViT', 240)\n",
        "\n",
        "    def train_forecasters(self, epochs=50):\n",
        "        print(\"Start Training\")\n",
        "        for epoch in range(epochs):\n",
        "            print(\"Epoch \", epoch)\n",
        "            self.ResNet_6.pre_steps(), self.ResNet_24.pre_steps(), self.ResNet_72.pre_steps(),\\\n",
        "                self.ResNet_120.pre_steps(), self.ResNet_240.pre_steps()\n",
        "            self.UNet_6.pre_steps(), self.UNet_24.pre_steps(), self.UNet_72.pre_steps(), self.UNet_120.pre_steps(),\\\n",
        "                self.UNet_240.pre_steps()\n",
        "            self.ViT_6.pre_steps(), self.ViT_24.pre_steps(), self.ViT_72.pre_steps(), self.ViT_120.pre_steps(),\\\n",
        "                self.ViT_240.pre_steps()\n",
        "\n",
        "            # New Part with Batch\n",
        "            print(\"   Train\")\n",
        "            for num, (train_6, train_24, train_72, train_120, train_240) in enumerate(self.train_loader):\n",
        "                # Train\n",
        "                printProgressAction('    Batch', num)\n",
        "                batch_dim = len(train_6[0])\n",
        "                train_constants = np.array((self.constants,) * batch_dim)\n",
        "\n",
        "                # Baselines\n",
        "                self.LinReg_Baseline_6.train_step(train_6, train_constants), \\\n",
        "                    self.LinReg_Baseline_24.train_step(train_24, train_constants), \\\n",
        "                    self.LinReg_Baseline_72.train_step(train_72, train_constants), \\\n",
        "                    self.LinReg_Baseline_120.train_step(train_120, train_constants), \\\n",
        "                    self.LinReg_Baseline_240.train_step(train_240, train_constants)\n",
        "                # ResNets\n",
        "                self.ResNet_6.train_step(train_6, train_constants),\\\n",
        "                    self.ResNet_24.train_step(train_24, train_constants),\\\n",
        "                    self.ResNet_72.train_step(train_72, train_constants),\\\n",
        "                    self.ResNet_120.train_step(train_120, train_constants),\\\n",
        "                    self.ResNet_240.train_step(train_240, train_constants)\n",
        "                # UNets\n",
        "                self.UNet_6.train_step(train_6, train_constants),\\\n",
        "                    self.UNet_24.train_step(train_24, train_constants),\\\n",
        "                    self.UNet_72.train_step(train_72, train_constants),\\\n",
        "                    self.UNet_120.train_step(train_120, train_constants),\\\n",
        "                    self.UNet_240.train_step(train_240, train_constants)\n",
        "                # ViTs\n",
        "                self.ViT_6.train_step(train_6, train_constants),\\\n",
        "                    self.ViT_24.train_step(train_24, train_constants),\\\n",
        "                    self.ViT_72.train_step(train_72, train_constants),\\\n",
        "                    self.ViT_120.train_step(train_120, train_constants),\\\n",
        "                    self.ViT_240.train_step(train_240, train_constants)\n",
        "                # Optimization\n",
        "                # ResNets\n",
        "                self.ResNet_6.post_steps(epoch), self.ResNet_24.post_steps(epoch), self.ResNet_72.post_steps(epoch),\\\n",
        "                    self.ResNet_120.post_steps(epoch), self.ResNet_240.post_steps(epoch)\n",
        "                # UNets\n",
        "                self.UNet_6.post_steps(epoch), self.UNet_24.post_steps(epoch), self.UNet_72.post_steps(epoch),\\\n",
        "                    self.UNet_120.post_steps(epoch), self.UNet_240.post_steps(epoch)\n",
        "                # ViTs\n",
        "                self.ViT_6.post_steps(epoch), self.ViT_24.post_steps(epoch), self.ViT_72.post_steps(epoch),\\\n",
        "                    self.ViT_120.post_steps(epoch), self.ViT_240.post_steps(epoch)\n",
        "            # Validation\n",
        "            print(\"\\n   Validation\")\n",
        "            for num, (val_6, val_24, val_72, val_120, val_240) in enumerate(self.validation_loader):\n",
        "                printProgressAction('    Batch', num)\n",
        "                batch_dim = len(val_6[0])\n",
        "                val_constants = np.array((self.constants,) * batch_dim)\n",
        "                # Baselines\n",
        "                self.LinReg_Baseline_6.val_step(val_6, val_constants),\\\n",
        "                    self.LinReg_Baseline_24.val_step(val_24, val_constants),\\\n",
        "                    self.LinReg_Baseline_72.val_step(val_72, val_constants),\\\n",
        "                    self.LinReg_Baseline_120.val_step(val_120, val_constants),\\\n",
        "                    self.LinReg_Baseline_240.val_step(val_240, val_constants)\n",
        "                # ResNets\n",
        "                self.ResNet_6.val_step(val_6, val_constants), self.ResNet_24.val_step(val_24, val_constants),\\\n",
        "                    self.ResNet_72.val_step(val_72, val_constants), self.ResNet_120.val_step(val_120, val_constants),\\\n",
        "                    self.ResNet_240.val_step(val_240, val_constants)\n",
        "                # UNets\n",
        "                self.UNet_6.val_step(val_6, val_constants), self.UNet_24.val_step(val_24, val_constants),\\\n",
        "                    self.UNet_72.val_step(val_72, val_constants), self.UNet_120.val_step(val_120, val_constants),\\\n",
        "                    self.UNet_240.val_step(val_240, val_constants)\n",
        "                # ViTs\n",
        "                self.ViT_6.val_step(val_6, val_constants), self.ViT_24.val_step(val_24, val_constants),\\\n",
        "                    self.ViT_72.val_step(val_72, val_constants), self.ViT_120.val_step(val_120, val_constants),\\\n",
        "                    self.ViT_240.val_step(val_240, val_constants)\n",
        "\n",
        "            # EarlyStopping\n",
        "            # Baselines\n",
        "            self.LinReg_Baseline_6.stopping(), self.LinReg_Baseline_24.stopping(), self.LinReg_Baseline_72.stopping(),\\\n",
        "                self.LinReg_Baseline_120.stopping(), self.LinReg_Baseline_240.stopping()\n",
        "            # ResNets\n",
        "            self.ResNet_6.stopping(), self.ResNet_24.stopping(), self.ResNet_72.stopping(), self.ResNet_120.stopping(),\\\n",
        "                self.ResNet_240.stopping()\n",
        "            # UNets\n",
        "            self.UNet_6.stopping(), self.UNet_24.stopping(), self.UNet_72.stopping(), self.UNet_120.stopping(),\\\n",
        "                self.UNet_240.stopping()\n",
        "            # ViTs\n",
        "            self.ViT_6.stopping(), self.ViT_24.stopping(), self.ViT_72.stopping(), self.ViT_120.stopping(),\\\n",
        "                self.ViT_240.stopping()\n",
        "\n",
        "            if all([self.ResNet_6.done, self.ResNet_24.done, self.ResNet_72.done, self.ResNet_120.done,\n",
        "                    self.ResNet_240.done, self.UNet_6.done, self.UNet_24.done, self.UNet_72.done, self.UNet_120.done,\n",
        "                    self.UNet_240.done, self.ViT_6.done, self.ViT_24.done, self.ViT_72.done, self.ViT_120.done,\n",
        "                    self.ViT_240.done]):\n",
        "                print(\"Stopped prematurely due to EarlyStopping\")\n",
        "                break\n",
        "        print(\"\\nEnd Training\")\n",
        "\n",
        "    def evaluate_forecasters(self, test_loader, constants_set):\n",
        "        print(\"Start Evaluation\")\n",
        "        for num, (test_6, test_24, test_72, test_120, test_240) in enumerate(test_loader):\n",
        "            if len(test_6[0]) != 128:\n",
        "                break\n",
        "            printProgressAction('    Batch', num)\n",
        "            batch_dim = len(test_6[0])\n",
        "            test_constants = np.array((constants_set,) * batch_dim)\n",
        "            # Baselines\n",
        "            self.LinReg_Baseline_6.eval_step(test_6[0], test_6[1], test_6[2], test_6[3][:, [4, 9, 33], :, :],\n",
        "                                             test_constants), \\\n",
        "                self.LinReg_Baseline_24.eval_step(test_24[0], test_24[1], test_24[2], test_24[3][:, [4, 9, 33], :, :],\n",
        "                                                  test_constants), \\\n",
        "                self.LinReg_Baseline_72.eval_step(test_72[0], test_72[1], test_72[2], test_72[3][:, [4, 9, 33], :, :],\n",
        "                                                  test_constants), \\\n",
        "                self.LinReg_Baseline_120.eval_step(test_120[0], test_120[1], test_120[2], test_120[3][:, [4, 9, 33], :, :],\n",
        "                                                   test_constants), \\\n",
        "                self.LinReg_Baseline_240.eval_step(test_240[0], test_240[1], test_240[2], test_240[3][:, [4, 9, 33], :, :],\n",
        "                                                   test_constants)\n",
        "            # ResNets\n",
        "            self.ResNet_6.eval_step(test_6[0], test_6[1], test_6[2], test_6[3][:, [4, 9, 33], :, :], test_constants),\\\n",
        "                self.ResNet_24.eval_step(test_24[0], test_24[1], test_24[2], test_24[3][:, [4, 9, 33], :, :],\n",
        "                                         test_constants),\\\n",
        "                self.ResNet_72.eval_step(test_72[0], test_72[1], test_72[2], test_72[3][:, [4, 9, 33], :, :],\n",
        "                                         test_constants),\\\n",
        "                self.ResNet_120.eval_step(test_120[0], test_120[1], test_120[2], test_120[3][:, [4, 9, 33], :, :],\n",
        "                                          test_constants),\\\n",
        "                self.ResNet_240.eval_step(test_240[0], test_240[1], test_240[2], test_240[3][:, [4, 9, 33], :, :],\n",
        "                                          test_constants)\n",
        "            # UNets\n",
        "            self.UNet_6.eval_step(test_6[0], test_6[1], test_6[2], test_6[3][:, [4, 9, 33], :, :], test_constants),\\\n",
        "                self.UNet_24.eval_step(test_24[0], test_24[1], test_24[2], test_24[3][:, [4, 9, 33], :, :],\n",
        "                                       test_constants),\\\n",
        "                self.UNet_72.eval_step(test_72[0], test_72[1], test_72[2], test_72[3][:, [4, 9, 33], :, :],\n",
        "                                       test_constants),\\\n",
        "                self.UNet_120.eval_step(test_120[0], test_120[1], test_120[2], test_120[3][:, [4, 9, 33], :, :],\n",
        "                                        test_constants),\\\n",
        "                self.UNet_240.eval_step(test_240[0], test_240[1], test_240[2], test_240[3][:, [4, 9, 33], :, :],\n",
        "                                        test_constants)\n",
        "            # ViTs\n",
        "            self.ViT_6.eval_step(test_6[0], test_6[1], test_6[2], test_6[3][:, [4, 9, 33], :, :],\n",
        "                                 test_constants),\\\n",
        "                self.ViT_24.eval_step(test_24[0], test_24[1], test_24[2], test_24[3][:, [4, 9, 33], :, :],\n",
        "                                      test_constants),\\\n",
        "                self.ViT_72.eval_step(test_72[0], test_72[1], test_72[2], test_72[3][:, [4, 9, 33], :, :],\n",
        "                                      test_constants),\\\n",
        "                self.ViT_120.eval_step(test_120[0], test_120[1], test_120[2], test_120[3][:, [4, 9, 33], :, :],\n",
        "                                       test_constants),\\\n",
        "                self.ViT_240.eval_step(test_240[0], test_240[1], test_240[2], test_240[3][:, [4, 9, 33], :, :],\n",
        "                                       test_constants)\n",
        "\n",
        "        # Baselines\n",
        "        rmse_LinReg_Baseline_6, acc_LinReg_Baseline_6 = compute_eval(self.LinReg_Baseline_6.pred,\n",
        "                                                                     self.LinReg_Baseline_6.targ, self.latitude_weights,\n",
        "                                                                     self.LinReg_Baseline_6.Net.set_climatology)\n",
        "        rmse_LinReg_Baseline_24, acc_LinReg_Baseline_24 = compute_eval(self.LinReg_Baseline_24.pred,\n",
        "                                                                       self.LinReg_Baseline_24.targ,\n",
        "                                                                       self.latitude_weights,\n",
        "                                                                       self.LinReg_Baseline_24.Net.set_climatology)\n",
        "        rmse_LinReg_Baseline_72, acc_LinReg_Baseline_72 = compute_eval(self.LinReg_Baseline_72.pred,\n",
        "                                                                       self.LinReg_Baseline_72.targ,\n",
        "                                                                       self.latitude_weights,\n",
        "                                                                       self.LinReg_Baseline_72.Net.set_climatology)\n",
        "        rmse_LinReg_Baseline_120, acc_LinReg_Baseline_120 = compute_eval(self.LinReg_Baseline_120.pred,\n",
        "                                                                         self.LinReg_Baseline_120.targ,\n",
        "                                                                         self.latitude_weights,\n",
        "                                                                         self.LinReg_Baseline_120.Net.set_climatology)\n",
        "        rmse_LinReg_Baseline_240, acc_LinReg_Baseline_240 = compute_eval(self.LinReg_Baseline_240.pred,\n",
        "                                                                         self.LinReg_Baseline_240.targ,\n",
        "                                                                         self.latitude_weights,\n",
        "                                                                         self.LinReg_Baseline_240.Net.set_climatology)\n",
        "        # ResNets\n",
        "        rmse_ResNet_6, acc_ResNet_6 = compute_eval(self.ResNet_6.pred, self.ResNet_6.targ, self.latitude_weights,\n",
        "                                                   self.ResNet_6.Net.set_climatology)\n",
        "        rmse_ResNet_24, acc_ResNet_24 = compute_eval(self.ResNet_24.pred, self.ResNet_24.targ, self.latitude_weights,\n",
        "                                                     self.ResNet_24.Net.set_climatology)\n",
        "        rmse_ResNet_72, acc_ResNet_72 = compute_eval(self.ResNet_72.pred, self.ResNet_72.targ, self.latitude_weights,\n",
        "                                                     self.ResNet_72.Net.set_climatology)\n",
        "        rmse_ResNet_120, acc_ResNet_120 = compute_eval(self.ResNet_120.pred, self.ResNet_120.targ,\n",
        "                                                       self.latitude_weights, self.ResNet_120.Net.set_climatology)\n",
        "        rmse_ResNet_240, acc_ResNet_240 = compute_eval(self.ResNet_240.pred, self.ResNet_240.targ,\n",
        "                                                       self.latitude_weights, self.ResNet_240.Net.set_climatology)\n",
        "        # UNets\n",
        "        rmse_UNet_6, acc_UNet_6 = compute_eval(self.UNet_6.pred, self.UNet_6.targ, self.latitude_weights,\n",
        "                                               self.UNet_6.Net.set_climatology)\n",
        "        rmse_UNet_24, acc_UNet_24 = compute_eval(self.UNet_24.pred, self.UNet_24.targ, self.latitude_weights,\n",
        "                                                 self.UNet_24.Net.set_climatology)\n",
        "        rmse_UNet_72, acc_UNet_72 = compute_eval(self.UNet_72.pred, self.UNet_72.targ, self.latitude_weights,\n",
        "                                                 self.UNet_72.Net.set_climatology)\n",
        "        rmse_UNet_120, acc_UNet_120 = compute_eval(self.UNet_120.pred, self.UNet_120.targ, self.latitude_weights,\n",
        "                                                   self.UNet_120.Net.set_climatology)\n",
        "        rmse_UNet_240, acc_UNet_240 = compute_eval(self.UNet_240.pred, self.UNet_240.targ, self.latitude_weights,\n",
        "                                                   self.UNet_240.Net.set_climatology)\n",
        "        # ViT\n",
        "        rmse_ViT_6, acc_ViT_6 = compute_eval(self.ViT_6.pred, self.ViT_6.targ, self.latitude_weights,\n",
        "                                             self.ViT_6.Net.set_climatology)\n",
        "        rmse_ViT_24, acc_ViT_24 = compute_eval(self.ViT_24.pred, self.ViT_24.targ, self.latitude_weights,\n",
        "                                               self.ViT_24.Net.set_climatology)\n",
        "        rmse_ViT_72, acc_ViT_72 = compute_eval(self.ViT_72.pred, self.ViT_72.targ, self.latitude_weights,\n",
        "                                               self.ViT_72.Net.set_climatology)\n",
        "        rmse_ViT_120, acc_ViT_120 = compute_eval(self.ViT_120.pred, self.ViT_120.targ, self.latitude_weights,\n",
        "                                                 self.ViT_120.Net.set_climatology)\n",
        "        rmse_ViT_240, acc_ViT_240 = compute_eval(self.ViT_240.pred, self.ViT_240.targ, self.latitude_weights,\n",
        "                                                 self.ViT_240.Net.set_climatology)\n",
        "\n",
        "        linreg_baseline_rmse = create_list([rmse_LinReg_Baseline_6, rmse_LinReg_Baseline_24, rmse_LinReg_Baseline_72,\n",
        "                                            rmse_LinReg_Baseline_120, rmse_LinReg_Baseline_240])\n",
        "        linreg_baseline_acc = create_list([acc_LinReg_Baseline_6, acc_LinReg_Baseline_24, acc_LinReg_Baseline_72,\n",
        "                                           acc_LinReg_Baseline_120, acc_LinReg_Baseline_240])\n",
        "        resnet_rmse = create_list([rmse_ResNet_6, rmse_ResNet_24, rmse_ResNet_72, rmse_ResNet_120, rmse_ResNet_240])\n",
        "        resnet_acc = create_list([acc_ResNet_6, acc_ResNet_24, acc_ResNet_72, acc_ResNet_120, acc_ResNet_240])\n",
        "        unet_rmse = create_list([rmse_UNet_6, rmse_UNet_24, rmse_UNet_72, rmse_UNet_120, rmse_UNet_240])\n",
        "        unet_acc = create_list([acc_UNet_6, acc_UNet_24, acc_UNet_72, acc_UNet_120, acc_UNet_240])\n",
        "        vit_rmse = create_list([rmse_ViT_6, rmse_ViT_24, rmse_ViT_72, rmse_ViT_120, rmse_ViT_240])\n",
        "        vit_acc = create_list([acc_ViT_6, acc_ViT_24, acc_ViT_72, acc_ViT_120, acc_ViT_240])\n",
        "\n",
        "        plot('t2m', linreg_baseline_rmse[0], linreg_baseline_acc[0], resnet_rmse[0], resnet_acc[0], unet_rmse[0],\n",
        "             unet_acc[0], vit_rmse[0], vit_acc[0])\n",
        "        plot('Z500', linreg_baseline_rmse[1], linreg_baseline_acc[1], resnet_rmse[1], resnet_acc[1], unet_rmse[1],\n",
        "             unet_acc[1], vit_rmse[1], vit_acc[1])\n",
        "        plot('T850', linreg_baseline_rmse[2], linreg_baseline_acc[2], resnet_rmse[2], resnet_acc[2], unet_rmse[2],\n",
        "             unet_acc[2], vit_rmse[2], vit_acc[2])\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.state_dict(), 'model.pt')\n",
        "\n",
        "    def load(self):\n",
        "        self.load_state_dict(torch.load('model.pt', map_location=self.device))\n",
        "\n",
        "    def to(self, device):\n",
        "        ret = super().to(device)\n",
        "        ret.device = device\n",
        "        return ret"
      ],
      "metadata": {
        "id": "2nTIy_x6frSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre\n",
        "In this section, you can change the parameters of the models to be trained and evaluated. You can also define the number of episodes for the training."
      ],
      "metadata": {
        "id": "zi-dJbwZHcc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_params = [32, 4]     # [128, 28]\n",
        "u_params = [16, 1]       # [64, 2]\n",
        "vit_params = [2, 2, 1, 32]     # [8, 4, 2, 128]"
      ],
      "metadata": {
        "id": "Iq8xnZIyGNo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ep = 1"
      ],
      "metadata": {
        "id": "i_MHsH5Vv11Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It creates an object that contains the models to be trained and the methods required for training and evaluation."
      ],
      "metadata": {
        "id": "y8WQ_HYTwmqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forecasters = Forecasting(constants_set, train_set, validation_set, res_params=res_params, u_params=u_params, vit_params=vit_params).to(device)"
      ],
      "metadata": {
        "id": "hxEOATlOG-QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "Here the models are trained"
      ],
      "metadata": {
        "id": "zL4XOZc6hF96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forecasters.train_forecasters(epochs=ep)"
      ],
      "metadata": {
        "id": "Y87-lm5xhPvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "This part defines the test DataLoader and then evaluates the models"
      ],
      "metadata": {
        "id": "mugpMzXchfAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_6 = [test_set[12:4374], test_set[6:4368], test_set[0:4362], test_set[18:4380]]\n",
        "t_24 = [test_set[12:4356], test_set[6:4350], test_set[0:4344], test_set[36:4380]]\n",
        "t_72 = [test_set[12:4308], test_set[6:4302], test_set[0:4296], test_set[84:4380]]\n",
        "t_120 = [test_set[12:4260], test_set[6:4254], test_set[0:4248], test_set[132:4380]]\n",
        "t_240 = [test_set[12:4140], test_set[6:4134], test_set[0:4128], test_set[252:4380]]\n",
        "t = CustomDataset(t_6, t_24, t_72, t_120, t_240)\n",
        "test_loader = torch.utils.data.DataLoader(t, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "HzKrdHcBHVKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecasters.eva_forecasters(test_loader, constants_set)"
      ],
      "metadata": {
        "id": "E3v_Rwtiheql"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}